% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clusterSVGsBayes.R
\name{clusterSVGsBayes}
\alias{clusterSVGsBayes}
\title{Cluster SVGs into modules.}
\usage{
clusterSVGsBayes(
  sp.obj = NULL,
  svgs = NULL,
  n.clust = 5L,
  n.PCs = 30L,
  algorithm = "meanfield",
  n.iter = 3000,
  n.draws = 1000L,
  elbo.samples = NULL,
  opencl.params = NULL,
  n.cores = 2L,
  random.seed = 312,
  verbose = TRUE
)
}
\arguments{
\item{sp.obj}{An object of class \code{Seurat} or \code{SpatialExperiment} upon which \code{\link{findSpatiallyVariableFeaturesBayes}} and \code{\link{classifySVGs}} have been run. Defaults to NULL.}

\item{svgs}{A character vector containing the identified SVGs. Defaults to NULL.}

\item{n.clust}{An integer specifying the number of clusters to fit to the data. Defaults to 5.}

\item{n.PCs}{An integer specifying the number of principal components (PCs) to reduce the data to prior to clustering. Defaults to 30.}

\item{algorithm}{A string specifying the variational inference (VI) approximation algorithm to be used. Must be one of "meanfield", "fullrank", or "pathfinder". Defaults to "meanfield".}

\item{n.iter}{An integer specifying the maximum number of iterations. Defaults to 3000.}

\item{n.draws}{An integer specifying the number of draws to be generated from the variational posterior. Defaults to 1000.}

\item{elbo.samples}{An integer specifying the number of samples to be used to estimate the ELBO at every 100th iteration. Higher values will provide a more accurate estimate at the cost of computational complexity. Defaults to 150 when \code{algorithm} is one of "meanfield" or "fullrank", and 50 when \code{algorithm} is "pathfinder".}

\item{opencl.params}{A two-element double vector specifying the platform and device IDs of the OpenCL GPU device. Most users should specify \code{c(0, 0)}. See \code{\link[brms]{opencl}} for more details. Defaults to NULL.}

\item{n.cores}{An integer specifying the number of threads used in compiling and fitting the model and estimating the soft cluster assignment probabilities. Defaults to 2.}

\item{random.seed}{A double specifying the random seed to be used when fitting and sampling from the model. Defaults to 312.}

\item{verbose}{A Boolean specifying whether or not verbose model output should be printed to the console. Defaults to TRUE.}
}
\value{
A list containing the gene-level PCA embedding, a \code{data.frame} of the soft cluster assignments, the fitted model from \code{\link[cmdstanr]{cmdstan_model}}, and the estimated log-likelihood and BIC of the model.
}
\description{
This downstream analysis function clusters identified SVGs into modules via an approximate Bayesian soft-clustering approach.
}
\details{
\itemize{
\item The soft clustering algorithm is a Gaussian mixture model (GMM), thus each cluster is modeled by a multivariate normal distribution with diagonal covariance. The diagonal covariance assumption is appropriate because each PC is orthogonal to the others.
\item The mixing proportions for each cluster are specified with a Dirichlet prior, while the mean follows a Gaussian distribution and the standard deviation a HalfGaussian.
\item Due to the architecture of the model, it is necessary for the user to supply a number of clusters via the \code{n.clust} argument. It s difficult to know the correct value to provide beforehand, but luckily the clustering model is quick to run and so multiple values of \code{n.clust} can be fitted and visualized in order to find the "best" value.
\item After modeling, the per-cluster assignment probabilities are computed for each gene. The cluster with the highest probability is then defined as the hard cluster for each gene.
\item The cluster model also estimated the log-likelihood per-gene, per-draw. This is then used to estimate the overall log-likelihood of the model as well as the corresponding Bayesian information criterion (BIC). The BIC can be used to compare multiple runs of the model, and thus choose the best value of the number of clusters \code{n.clust}.
}
}
\examples{
data(seu_brain)
seu_brain <- Seurat::NormalizeData(seu_brain, verbose = FALSE) \%>\% 
             Seurat::FindVariableFeatures(nfeatures = 1000L, verbose = FALSE)
seu_brain <- findSpatiallyVariableFeaturesBayes(seu_brain,
                                                naive.hvgs = Seurat::VariableFeatures(seu_brain),
                                                kernel = "matern",
                                                kernel.smoothness = 1.5,
                                                algorithm = "meanfield",
                                                n.cores = 1L,
                                                save.model = TRUE) \%>\%
             classifySVGs(n.SVG = 200L)
svg_clusters <- clusterSVGsBayes(seu_brain,
                                 svgs = Seurat::VariableFeatures(seu_brain),
                                 n.clust = 2L,
                                 n.cores = 1L)
}
\author{
Jack R. Leary
}
